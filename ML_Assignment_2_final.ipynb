{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0Z85vs1DlLoV"
      },
      "outputs": [],
      "source": [
        "#Training and testing models on 20newsgroups dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sYsja0jHKgfE"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups #Fetching 20newsgroups\n",
        "\n",
        "X_train, y_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), return_X_y=True)\n",
        "X_test, y_test  = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), return_X_y=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d19V6vbkQW0",
        "outputId": "81fbb43b-7f7f-4744-8acf-0faadecfd53e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "Predicting labels\n",
            "F1 micro: 0.6771109930961232\n",
            "Hamming loss: 0.3228890069038768\n",
            "Accuracy: 0.6771109930961232 \n"
          ]
        }
      ],
      "source": [
        "#PROBABILISTIC MODEL\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multiclass import OneVsRestClassifier \n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "#Vectorizing for Multinominal model\n",
        "vectorizer = CountVectorizer(stop_words=\"english\", ngram_range=(1, 3))\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "#training and predicting labels with probablistic model\n",
        "classifier = OneVsRestClassifier(MultinomialNB(alpha=0.1))\n",
        "print(\"Training the model...\")\n",
        "classifier.fit(X_train, y_train)\n",
        "print(\"Predicting labels\")\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "print('F1 micro:', f1_score(y_test, predictions, average='micro'))\n",
        "print('Hamming loss:', hamming_loss(y_test, predictions))\n",
        "print('Accuracy: %s ' %(accuracy_score(predictions, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZqA8J-BgKs5h"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups #Fetching 20newsgroups\n",
        "\n",
        "X_train, y_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), return_X_y=True)\n",
        "X_test, y_test  = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), return_X_y=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL8DSXE0k6e1",
        "outputId": "2bceb473-c22e-451d-cfe1-a09430ca54b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "Predicting labels\n",
            "F1 micro: 0.7030005310674455\n",
            "Hamming loss: 0.2969994689325544\n",
            "Accuracy: 0.7030005310674455 \n"
          ]
        }
      ],
      "source": [
        "# NON-PROBABILISTIC MODEL\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  #vectorizing for SVC model\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier \n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, ngram_range=(1, 3))\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "#training and predicting labels with non-probablistic model\n",
        "classifier = OneVsRestClassifier(LinearSVC())\n",
        "\n",
        "print(\"Training the model...\")\n",
        "classifier.fit(X_train, y_train)\n",
        "print(\"Predicting labels\")\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "print('F1 micro:', f1_score(y_test, predictions, average='micro', labels=np.unique(predictions)))\n",
        "print('Hamming loss:', hamming_loss(y_test, predictions))\n",
        "print('Accuracy: %s ' %(accuracy_score(predictions, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BcXWsecDlDmr"
      },
      "outputs": [],
      "source": [
        "#Training and testing models on Reuters dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "375TCNM2fXPa",
        "outputId": "6f3a7ce4-b401-4f16-927e-84e48f14615a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-06-11 15:28:31--  https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz\n",
            "Resolving kdd.ics.uci.edu (kdd.ics.uci.edu)... 128.195.1.86\n",
            "Connecting to kdd.ics.uci.edu (kdd.ics.uci.edu)|128.195.1.86|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘reuters21578.tar.gz’ not modified on server. Omitting download.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -N https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz #Fetching reuters21578\n",
        "dataset = !tar xvzf reuters21578.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajYWTPWdHF6T",
        "outputId": "57ddd3cd-3436-453b-d009-bff9b8be38ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: reut2-000.sgm\n",
            "Processing: reut2-001.sgm\n",
            "Processing: reut2-002.sgm\n",
            "Processing: reut2-003.sgm\n",
            "Processing: reut2-004.sgm\n",
            "Processing: reut2-005.sgm\n",
            "Processing: reut2-006.sgm\n",
            "Processing: reut2-007.sgm\n",
            "Processing: reut2-008.sgm\n",
            "Processing: reut2-009.sgm\n",
            "Processing: reut2-010.sgm\n",
            "Processing: reut2-011.sgm\n",
            "Processing: reut2-012.sgm\n",
            "Processing: reut2-013.sgm\n",
            "Processing: reut2-014.sgm\n",
            "Processing: reut2-015.sgm\n",
            "Processing: reut2-016.sgm\n",
            "Processing: reut2-017.sgm\n",
            "Processing: reut2-018.sgm\n",
            "Processing: reut2-019.sgm\n",
            "Processing: reut2-020.sgm\n",
            "Processing: reut2-021.sgm\n",
            "21578 21578\n"
          ]
        }
      ],
      "source": [
        "topicc = []   #Reading the files and creating two lists - with body/titles and topics\n",
        "contt = []\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "for f in dataset:\n",
        "  tags = []\n",
        "  if f.endswith('sgm'):\n",
        "    print(f'Processing: {f}')\n",
        "    lines = []\n",
        "    for line in open(f, 'rb').readlines():\n",
        "      line = line.decode('utf-8','ignore')\n",
        "      lines.append(line)\n",
        "    data = ' '.join(lines)\n",
        "    soup = BeautifulSoup(data, 'html.parser')\n",
        "    contents = soup.findAll('text')\n",
        "    labels = soup.findAll('topics')      \n",
        "\n",
        "    cont = []  #cleaning the dataset\n",
        "    for content in contents:\n",
        "        content = re.sub(r'\\d+',' ', content.text)\n",
        "        content = re.sub(r'[^\\w\\s]', ' ', content)\n",
        "        content = content.replace('Reuter',' ').replace('\\x02',' ').replace('\\n',' ')\n",
        "        cont.append(content)  \n",
        "\n",
        "    topics = []\n",
        "    for topic in labels:\n",
        "        topics.append(str(topic))\n",
        "\n",
        "    top = [] #splitting topics \n",
        "    for topic in topics:\n",
        "        topic = re.findall(r'<topics>(.*?)</topics>', str(topic).strip('\\n'))\n",
        "        if len(topic[0]) != 0:\n",
        "          topic = re.findall(r'<d>(.*?)</d>', str(topic))\n",
        "        top.append(topic)\n",
        "\n",
        "    # conti = cont.copy() #removing articles with no topics  \n",
        "    # topi = top.copy()\n",
        "    # for i, topic in enumerate(topi):\n",
        "    #     if len(topic[0]) == 0:\n",
        "    #       top.remove(topic)\n",
        "    #       cont.remove(conti[i])\n",
        "\n",
        "    topicc += top\n",
        "    contt += cont          \n",
        "\n",
        "\n",
        "print(len(contt), len(topicc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jfFlt_Jpl9Og"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict #creating dictionary with topics and corresponding articles (indexes)\n",
        "d = defaultdict(list)\n",
        "conti = contt.copy() \n",
        "topi = topicc.copy()\n",
        "for i, topic in enumerate(topi):\n",
        "    if len(topic) > 1:\n",
        "      for item in topic:\n",
        "        d[item].append(i)\n",
        "    else:\n",
        "      d[topic[0]].append(i)\n",
        "\n",
        "for t, a in d.items():\n",
        "        freq = len(d[t])\n",
        "        d[t] = (a, freq)\n",
        "\n",
        "rare_topics = d.copy()  #dictionary with rare topics\n",
        "rest_topics = d.copy()  #dictionary with the rest of the topics\n",
        "for word, v in d.items():\n",
        "    if d[word][1] >= 5:   \n",
        "      del rare_topics[word]\n",
        "    else:\n",
        "      del rest_topics[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ys78FWoynE2Q"
      },
      "outputs": [],
      "source": [
        "l = []    #removing topics with fewer than 5 articles\n",
        "for topic, v in rare_topics.items():\n",
        "  for elem in v[0]:\n",
        "    if elem not in l:\n",
        "      l.append(elem)\n",
        "    else:\n",
        "       pass\n",
        "\n",
        "for number in l:\n",
        "      topicc.remove(topi[number])\n",
        "      contt.remove(conti[number])       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVcKqjBUto3e",
        "outputId": "6855b585-4535-4e8b-b49f-9d71940208bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21499\n",
            "21499\n"
          ]
        }
      ],
      "source": [
        "print(len(contt)) #length of dataset after removing rare topics\n",
        "print(len(topicc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl2watTDPb4v",
        "outputId": "a60e7e89-3e7a-4030-aa35-2f95281cd9b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate:\tTraining set accuracy:\n",
            "100 78.31775103203675\n",
            "10 78.42194313622885\n",
            "1 78.87516716088145\n",
            "0.1 78.84051398337112\n",
            "0.01 78.86063143206\n",
            "0.001 78.92924007209722\n",
            "0.0001 78.95703238560381\n",
            "Best parameter:  0.0001\n"
          ]
        }
      ],
      "source": [
        "# Searching for the best hyperparameters for the probabilistic model\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(contt, topicc, test_size=0.2)\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words=\"english\", ngram_range=(1, 3))\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "m = MultiLabelBinarizer()\n",
        "y_train = m.fit_transform(y_train)\n",
        "y_test = m.transform(y_test)\n",
        "\n",
        "best_hyperparameters = None\n",
        "print(\"Learning rate:\\tTraining set accuracy:\")\n",
        "\n",
        "params = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "for value in params:\n",
        "    model = OneVsRestClassifier(MultinomialNB(alpha = value))\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    training_accuracy = np.sum(model.predict(X_train)==y_train)/len(y_train)\n",
        "\n",
        "    if best_hyperparameters is None or best_hyperparameters[1] < training_accuracy:\n",
        "        best_hyperparameters = (value, training_accuracy)\n",
        "    print(value, training_accuracy)\n",
        "\n",
        "best_param = best_hyperparameters[0]\n",
        "print('Best parameter: ', (best_param))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dHFLK6RHSc0",
        "outputId": "96ef5397-8bd1-4f78-d14a-78fd4e52673f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:987: UserWarning: unknown class(es) ['sun-oil'] will be ignored\n",
            "  .format(sorted(unknown, key=str)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "Predicting labels\n",
            "F1 micro: 0.8370279146141214\n",
            "Hamming loss: 0.005793082886106142\n",
            "Accuracy: 72.72093023255813 \n"
          ]
        }
      ],
      "source": [
        "#PROBABILISTIC MODEL\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multiclass import OneVsRestClassifier \n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "#Splitting dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(contt, topicc, test_size=0.2)\n",
        "#Vectorizing for Multinominal model\n",
        "vectorizer = CountVectorizer(stop_words=\"english\", ngram_range=(1, 3))\n",
        "#vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, ngram_range=(1, 3))\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "m = MultiLabelBinarizer()\n",
        "y_train = m.fit_transform(y_train)\n",
        "y_test = m.transform(y_test)\n",
        "\n",
        "#training and predicting labels with probablistic model\n",
        "classifier = OneVsRestClassifier(MultinomialNB(alpha=best_param))\n",
        "print(\"Training the model...\")\n",
        "classifier.fit(X_train, y_train)\n",
        "print(\"Predicting labels\")\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "\n",
        "print('F1 micro:', f1_score(y_test, y_pred, average='micro', labels=np.unique(y_pred)))\n",
        "print('Hamming loss:', hamming_loss(y_test, y_pred))\n",
        "print('Accuracy: %s ' %(accuracy_score(y_pred, y_test)*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsKJ2_PqVGGF",
        "outputId": "f3f16f2f-d9e3-42de-fa89-495f470c99d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate:\tTraining set accuracy:\n",
            "20 78.99465085179371\n",
            "10 78.99453456596314\n",
            "5 78.99412756555614\n",
            "1 78.98912727484156\n",
            "0.1 78.63963021105879\n",
            "0.01 78.32234432234432\n",
            "0.001 78.25018896447467\n",
            "Best parameter:  20\n"
          ]
        }
      ],
      "source": [
        "# Searching for the best hyperparameters for the non-probabilistic model\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(contt, topicc, test_size=0.2)\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, ngram_range=(1, 3))\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "m = MultiLabelBinarizer()\n",
        "y_train = m.fit_transform(y_train)\n",
        "y_test = m.transform(y_test)\n",
        "\n",
        "best_hyperparameters = None\n",
        "print(\"Learning rate:\\tTraining set accuracy:\")\n",
        "\n",
        "param = [20, 10, 5, 1, 0.1, 0.01, 0.001]\n",
        "for value in param:\n",
        "    model = OneVsRestClassifier(LinearSVC(C = value))\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    training_accuracy = np.sum(model.predict(X_train)==y_train)/len(y_train)\n",
        "\n",
        "    if best_hyperparameters is None or best_hyperparameters[1] < training_accuracy:\n",
        "        best_hyperparameters = (value, training_accuracy)\n",
        "    print(value, training_accuracy)\n",
        "\n",
        "best_paramNP = best_hyperparameters[0]\n",
        "print('Best parameter: ', (best_paramNP))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fNuIOrq7-AB",
        "outputId": "5ed44a53-a4a5-480b-c09f-0fa2980cd0ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "Predicting labels\n",
            "F1 micro: 0.8841166936790924\n",
            "Hamming loss: 0.004038857815719753\n",
            "Accuracy: 80.48837209302326 \n"
          ]
        }
      ],
      "source": [
        "# NON-PROBABILISTIC MODEL\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  #vectorizing for SVC model\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier \n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "\n",
        "#Splitting dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(contt, topicc, test_size=0.2)\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, ngram_range=(1, 3))\n",
        "#vectorizer = CountVectorizer(stop_words=\"english\", ngram_range=(1, 3))\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "m = MultiLabelBinarizer()\n",
        "y_train = m.fit_transform(y_train)\n",
        "y_test = m.transform(y_test)\n",
        "\n",
        "#training and predicting labels with non-probablistic model\n",
        "classifier = OneVsRestClassifier(LinearSVC(C = best_paramNP))\n",
        "\n",
        "print(\"Training the model...\")\n",
        "classifier.fit(X_train, y_train)\n",
        "print(\"Predicting labels\")\n",
        "y_pred = classifier.predict(X_test)\n",
        "print('F1 micro:', f1_score(y_test, y_pred, average='micro', labels=np.unique(y_pred)))\n",
        "print('Hamming loss:', hamming_loss(y_test, y_pred))\n",
        "print('Accuracy: %s ' %(accuracy_score(y_pred, y_test)*100))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ML Assignment 2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
